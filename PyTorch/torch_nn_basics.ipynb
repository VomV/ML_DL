{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Neural Network by subclassing nn.Module\n",
    "\n",
    "Implement the operations on input data in forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "                                                nn.Linear(28*28, 512),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(512, 512),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(512, 10),\n",
    "                                            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4309, 0.3866, 0.9199, 0.6738, 0.5324, 0.6543, 0.7047, 0.1817,\n",
       "          0.5209, 0.1916, 0.6448, 0.7683, 0.1728, 0.6307, 0.2201, 0.6140,\n",
       "          0.3486, 0.2821, 0.8802, 0.3718, 0.3747, 0.8448, 0.7826, 0.1113,\n",
       "          0.1735, 0.4839, 0.0096, 0.5959],\n",
       "         [0.4412, 0.4205, 0.1117, 0.3860, 0.6082, 0.7323, 0.0991, 0.5928,\n",
       "          0.1278, 0.3668, 0.9016, 0.7525, 0.2069, 0.4935, 0.0573, 0.8584,\n",
       "          0.5149, 0.7826, 0.8102, 0.9803, 0.7765, 0.1051, 0.8351, 0.1980,\n",
       "          0.7310, 0.3803, 0.9461, 0.9864],\n",
       "         [0.3298, 0.3257, 0.2081, 0.2162, 0.6965, 0.3753, 0.0387, 0.9535,\n",
       "          0.7369, 0.6591, 0.9137, 0.6023, 0.1439, 0.3817, 0.5558, 0.3662,\n",
       "          0.2674, 0.0190, 0.2688, 0.5340, 0.9470, 0.4960, 0.0852, 0.6926,\n",
       "          0.0653, 0.3890, 0.6029, 0.4240],\n",
       "         [0.5949, 0.3079, 0.4993, 0.2983, 0.3208, 0.9593, 0.2615, 0.6153,\n",
       "          0.3768, 0.8649, 0.0612, 0.5778, 0.6882, 0.3451, 0.6764, 0.9164,\n",
       "          0.0869, 0.2258, 0.4438, 0.9103, 0.3527, 0.4065, 0.1256, 0.1150,\n",
       "          0.0295, 0.0563, 0.9718, 0.1232],\n",
       "         [0.5405, 0.8469, 0.9178, 0.6876, 0.4669, 0.5990, 0.1818, 0.3323,\n",
       "          0.8232, 0.4335, 0.6862, 0.0013, 0.3510, 0.4598, 0.1414, 0.4556,\n",
       "          0.9764, 0.0711, 0.8202, 0.7155, 0.6034, 0.5427, 0.3221, 0.6087,\n",
       "          0.6228, 0.0582, 0.8287, 0.3758],\n",
       "         [0.3286, 0.2493, 0.7247, 0.6444, 0.1198, 0.9638, 0.9842, 0.8551,\n",
       "          0.0214, 0.2352, 0.1317, 0.8207, 0.8223, 0.7444, 0.8525, 0.9041,\n",
       "          0.8206, 0.2020, 0.7626, 0.2469, 0.6660, 0.1838, 0.1990, 0.8804,\n",
       "          0.6818, 0.1684, 0.9191, 0.3248],\n",
       "         [0.6019, 0.7509, 0.6305, 0.0415, 0.9873, 0.3147, 0.2798, 0.6107,\n",
       "          0.9821, 0.7679, 0.9739, 0.7832, 0.5510, 0.4453, 0.4023, 0.3230,\n",
       "          0.4102, 0.5389, 0.7912, 0.1432, 0.8120, 0.7615, 0.3895, 0.8809,\n",
       "          0.0022, 0.5034, 0.3229, 0.5887],\n",
       "         [0.5577, 0.8621, 0.2831, 0.1619, 0.7508, 0.2395, 0.1753, 0.1968,\n",
       "          0.2735, 0.6806, 0.2483, 0.9281, 0.8055, 0.9217, 0.3156, 0.2502,\n",
       "          0.8030, 0.0345, 0.1802, 0.6589, 0.2380, 0.4181, 0.3984, 0.8460,\n",
       "          0.6769, 0.1406, 0.8579, 0.8151],\n",
       "         [0.9934, 0.0103, 0.0270, 0.6838, 0.8998, 0.1866, 0.0716, 0.7814,\n",
       "          0.3102, 0.2621, 0.7534, 0.2675, 0.1856, 0.8406, 0.8250, 0.4020,\n",
       "          0.4719, 0.0821, 0.0207, 0.8256, 0.9569, 0.1062, 0.8772, 0.7339,\n",
       "          0.9633, 0.4379, 0.9602, 0.9174],\n",
       "         [0.0351, 0.2539, 0.2135, 0.5580, 0.5989, 0.2915, 0.2336, 0.2781,\n",
       "          0.9531, 0.3749, 0.3310, 0.7334, 0.8774, 0.0228, 0.9176, 0.0392,\n",
       "          0.6631, 0.7080, 0.2322, 0.2782, 0.3223, 0.9007, 0.1921, 0.2603,\n",
       "          0.8404, 0.1894, 0.1336, 0.2317],\n",
       "         [0.2568, 0.2796, 0.8716, 0.1244, 0.9811, 0.4757, 0.2576, 0.0350,\n",
       "          0.3347, 0.9454, 0.7256, 0.9433, 0.1102, 0.9099, 0.2441, 0.0974,\n",
       "          0.3644, 0.6297, 0.3705, 0.5360, 0.1915, 0.7591, 0.6303, 0.5059,\n",
       "          0.2697, 0.7260, 0.8848, 0.4668],\n",
       "         [0.8404, 0.9106, 0.2242, 0.2485, 0.1652, 0.9208, 0.3055, 0.0085,\n",
       "          0.8960, 0.7284, 0.4130, 0.1000, 0.0112, 0.1282, 0.9950, 0.5762,\n",
       "          0.9483, 0.9197, 0.3138, 0.1595, 0.9677, 0.1609, 0.1426, 0.9350,\n",
       "          0.2575, 0.1684, 0.0733, 0.1557],\n",
       "         [0.5183, 0.9475, 0.9977, 0.2506, 0.3605, 0.3555, 0.2941, 0.8642,\n",
       "          0.1726, 0.6229, 0.4843, 0.2033, 0.8161, 0.8411, 0.8298, 0.0316,\n",
       "          0.5186, 0.5920, 0.5092, 0.3478, 0.7721, 0.3074, 0.9959, 0.0606,\n",
       "          0.2194, 0.4612, 0.3630, 0.5781],\n",
       "         [0.0907, 0.3761, 0.1292, 0.2343, 0.5842, 0.1044, 0.4009, 0.6061,\n",
       "          0.4597, 0.9571, 0.4258, 0.4209, 0.8919, 0.5333, 0.8074, 0.3604,\n",
       "          0.8410, 0.8654, 0.0521, 0.1270, 0.3587, 0.5940, 0.7251, 0.0266,\n",
       "          0.7296, 0.7020, 0.3991, 0.5156],\n",
       "         [0.1944, 0.2928, 0.0699, 0.3539, 0.2991, 0.6678, 0.4456, 0.0989,\n",
       "          0.4517, 0.8672, 0.3985, 0.6363, 0.8599, 0.7250, 0.3869, 0.5795,\n",
       "          0.5506, 0.3916, 0.5958, 0.8023, 0.5983, 0.4828, 0.1613, 0.7039,\n",
       "          0.6681, 0.6235, 0.7542, 0.3384],\n",
       "         [0.7469, 0.0069, 0.0534, 0.3772, 0.7693, 0.4375, 0.0419, 0.8884,\n",
       "          0.6018, 0.5716, 0.3269, 0.7257, 0.0739, 0.7221, 0.1067, 0.3254,\n",
       "          0.4476, 0.9738, 0.2810, 0.4036, 0.5390, 0.0081, 0.8206, 0.2421,\n",
       "          0.2613, 0.2436, 0.9050, 0.5862],\n",
       "         [0.6298, 0.1784, 0.5298, 0.5319, 0.5852, 0.7571, 0.0992, 0.7255,\n",
       "          0.8771, 0.6322, 0.6896, 0.8559, 0.0326, 0.2269, 0.4737, 0.5495,\n",
       "          0.5548, 0.1076, 0.6388, 0.6312, 0.5205, 0.6447, 0.4554, 0.1120,\n",
       "          0.0347, 0.8413, 0.2020, 0.1259],\n",
       "         [0.7727, 0.7708, 0.4839, 0.8111, 0.0524, 0.0282, 0.8425, 0.8093,\n",
       "          0.3116, 0.6526, 0.3904, 0.1807, 0.9703, 0.0637, 0.9377, 0.7945,\n",
       "          0.1208, 0.5337, 0.4930, 0.1354, 0.3423, 0.8130, 0.0433, 0.1441,\n",
       "          0.6579, 0.0321, 0.5480, 0.9756],\n",
       "         [0.7144, 0.5087, 0.0410, 0.8812, 0.2215, 0.0739, 0.4990, 0.4218,\n",
       "          0.3177, 0.6585, 0.2298, 0.4940, 0.0517, 0.0731, 0.3585, 0.6193,\n",
       "          0.0961, 0.1033, 0.3546, 0.8728, 0.7557, 0.5048, 0.4931, 0.0258,\n",
       "          0.5204, 0.0277, 0.2001, 0.4078],\n",
       "         [0.2954, 0.1939, 0.9650, 0.8163, 0.3923, 0.7855, 0.6272, 0.7190,\n",
       "          0.6360, 0.2138, 0.5561, 0.4948, 0.2518, 0.0764, 0.8364, 0.3013,\n",
       "          0.5371, 0.6149, 0.8339, 0.7025, 0.3601, 0.1223, 0.1627, 0.4617,\n",
       "          0.6333, 0.1199, 0.9586, 0.9046],\n",
       "         [0.0457, 0.8332, 0.3593, 0.9739, 0.0362, 0.5116, 0.4359, 0.4707,\n",
       "          0.4141, 0.6399, 0.2910, 0.7216, 0.8059, 0.0600, 0.7643, 0.1053,\n",
       "          0.8626, 0.4428, 0.4822, 0.6295, 0.6044, 0.0516, 0.6076, 0.2192,\n",
       "          0.4849, 0.4083, 0.9518, 0.2698],\n",
       "         [0.5928, 0.8468, 0.2211, 0.8181, 0.5472, 0.6168, 0.4272, 0.2393,\n",
       "          0.6785, 0.3602, 0.9859, 0.6753, 0.4022, 0.9432, 0.4441, 0.8966,\n",
       "          0.1440, 0.5770, 0.6515, 0.9124, 0.6365, 0.1067, 0.7799, 0.7708,\n",
       "          0.1330, 0.9955, 0.5320, 0.7509],\n",
       "         [0.3134, 0.5524, 0.4495, 0.5140, 0.1069, 0.4485, 0.1220, 0.0598,\n",
       "          0.1694, 0.1302, 0.7642, 0.2377, 0.7259, 0.0622, 0.6960, 0.3752,\n",
       "          0.1132, 0.2314, 0.5535, 0.3177, 0.3696, 0.7768, 0.7321, 0.3062,\n",
       "          0.3526, 0.2516, 0.9019, 0.5919],\n",
       "         [0.7019, 0.1866, 0.1009, 0.5945, 0.1309, 0.9595, 0.9737, 0.7057,\n",
       "          0.4570, 0.0949, 0.9568, 0.3549, 0.6039, 0.8697, 0.0292, 0.5567,\n",
       "          0.0424, 0.2317, 0.5789, 0.7372, 0.8607, 0.7774, 0.4248, 0.0290,\n",
       "          0.6050, 0.4663, 0.0193, 0.6433],\n",
       "         [0.9032, 0.7808, 0.0439, 0.0716, 0.1886, 0.8418, 0.0547, 0.2694,\n",
       "          0.9471, 0.2524, 0.4872, 0.8943, 0.4186, 0.2923, 0.9424, 0.4168,\n",
       "          0.0194, 0.1275, 0.7237, 0.0455, 0.1416, 0.7110, 0.6164, 0.9338,\n",
       "          0.2225, 0.6314, 0.3676, 0.2980],\n",
       "         [0.3473, 0.9053, 0.0233, 0.8269, 0.3051, 0.0701, 0.6691, 0.6111,\n",
       "          0.6157, 0.2917, 0.3243, 0.5351, 0.4302, 0.4457, 0.0635, 0.1131,\n",
       "          0.6791, 0.0028, 0.7884, 0.4878, 0.2045, 0.2340, 0.0900, 0.5861,\n",
       "          0.0854, 0.7957, 0.5693, 0.3306],\n",
       "         [0.3555, 0.9849, 0.8271, 0.3812, 0.4297, 0.9543, 0.2242, 0.0925,\n",
       "          0.3593, 0.4762, 0.3586, 0.5639, 0.6755, 0.4539, 0.1594, 0.9729,\n",
       "          0.3169, 0.5698, 0.2469, 0.4160, 0.6957, 0.4887, 0.7038, 0.8018,\n",
       "          0.1965, 0.9899, 0.1241, 0.6525],\n",
       "         [0.7427, 0.4918, 0.4228, 0.9512, 0.3185, 0.1129, 0.6969, 0.1127,\n",
       "          0.2266, 0.6647, 0.3413, 0.3484, 0.2469, 0.8095, 0.6628, 0.6978,\n",
       "          0.7586, 0.8690, 0.8536, 0.1660, 0.6237, 0.4164, 0.1267, 0.4750,\n",
       "          0.2712, 0.4147, 0.5382, 0.3170]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0640,  0.0018,  0.0910,  0.0696, -0.0322, -0.0187,  0.0259,  0.0091,\n",
       "          0.0514,  0.0253]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1035, 0.0973, 0.1063, 0.1041, 0.0940, 0.0953, 0.0997, 0.0980, 0.1022,\n",
       "         0.0996]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(X)\n",
    "pred = nn.Softmax(dim=1)(logits)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pred.argmax(1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to data as it passes through layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 784])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img = torch.rand(3, 28, 28)\n",
    "\n",
    "flat_img = nn.Flatten()(input_img)\n",
    "flat_img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_1 = nn.Linear(28*28, 50)\n",
    "hidden_1 = layer_1(flat_img)\n",
    "hidden_1.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying non linearity with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0807, 0.4021, 0.0000, 0.0000, 0.0000, 0.0000, 0.4829, 0.0000, 0.0960,\n",
       "         0.0000, 0.2729, 0.0000, 0.0883, 0.0000, 0.1603, 0.0000, 0.1913, 0.1415,\n",
       "         0.0000, 0.3949, 0.1296, 0.0000, 0.2271, 0.2227, 0.2195, 0.0473, 0.0000,\n",
       "         0.0959, 0.2223, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1646,\n",
       "         0.1907, 0.5544, 0.0000, 0.0000, 0.1303, 0.1831, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.6671, 0.0000, 0.0731, 0.0000],\n",
       "        [0.0730, 0.2559, 0.0000, 0.0000, 0.0000, 0.0000, 0.4693, 0.1549, 0.3645,\n",
       "         0.0000, 0.0000, 0.0000, 0.2338, 0.0000, 0.4889, 0.0000, 0.0000, 0.0677,\n",
       "         0.0000, 0.7933, 0.3388, 0.0062, 0.5563, 0.2714, 0.3681, 0.0000, 0.0000,\n",
       "         0.3401, 0.0000, 0.0000, 0.0000, 0.1547, 0.0000, 0.0000, 0.0000, 0.3498,\n",
       "         0.0000, 0.3643, 0.0000, 0.0000, 0.0000, 0.1349, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.5113, 0.0000, 0.0175, 0.0000],\n",
       "        [0.0000, 0.1875, 0.0000, 0.0000, 0.0000, 0.0000, 0.5675, 0.1027, 0.1116,\n",
       "         0.0000, 0.0000, 0.0000, 0.0875, 0.0000, 0.4042, 0.0000, 0.3605, 0.0000,\n",
       "         0.0000, 0.7339, 0.0059, 0.0000, 0.2982, 0.2666, 0.0479, 0.1853, 0.0000,\n",
       "         0.1586, 0.0000, 0.0000, 0.0000, 0.1275, 0.0000, 0.0000, 0.3818, 0.0000,\n",
       "         0.1943, 0.4979, 0.0000, 0.0000, 0.0432, 0.0553, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 1.0258, 0.0684, 0.4945, 0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_1 = nn.ReLU()(hidden_1)\n",
    "hidden_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential is an ordered container of modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_mods = nn.Sequential(\n",
    "                          nn.Flatten(),\n",
    "                          layer_1,\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(50, 10)      \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last layer returns logits in the range -inf, inf. nn.Softmax scales these to 0,1 and dim specified is the one along which prob sum upto 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probs = softmax(logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model params: weights & biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subclassing nn.Module makes all parameters accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_relu_stack.0.weight torch.Size([512, 784]) tensor([[ 0.0343, -0.0215,  0.0252,  ...,  0.0135, -0.0194, -0.0216],\n",
      "        [-0.0146, -0.0254, -0.0012,  ...,  0.0282,  0.0168,  0.0287]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "linear_relu_stack.0.bias torch.Size([512]) tensor([0.0243, 0.0313], grad_fn=<SliceBackward0>)\n",
      "linear_relu_stack.2.weight torch.Size([512, 512]) tensor([[-0.0220,  0.0330, -0.0284,  ...,  0.0270, -0.0205,  0.0334],\n",
      "        [-0.0163, -0.0312, -0.0359,  ..., -0.0224,  0.0084,  0.0006]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "linear_relu_stack.2.bias torch.Size([512]) tensor([-0.0369, -0.0174], grad_fn=<SliceBackward0>)\n",
      "linear_relu_stack.4.weight torch.Size([10, 512]) tensor([[ 0.0062, -0.0336, -0.0198,  ...,  0.0386, -0.0246,  0.0079],\n",
      "        [ 0.0331, -0.0091, -0.0257,  ...,  0.0195,  0.0084, -0.0075]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "linear_relu_stack.4.bias torch.Size([10]) tensor([-0.0439, -0.0087], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size(), param[:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dddab157190c3c29c7bfa9724dd2612e80e7d4a281bb7f76e54f36d2e23abd8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
