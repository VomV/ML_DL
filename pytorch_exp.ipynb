{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "- To solve the problem of covariate shift and train NNs faster. Also applicable for sequential networks unlike batch norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinton 2016"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a\n",
    "mean and variance which are then used to normalize the summed input to that\n",
    "neuron on each training case. \n",
    "\n",
    "- This significantly reduces the training time in feedforward neural networks. However, the effect of batch normalization is dependent\n",
    "on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. \n",
    "\n",
    "- Transpose batch normalization into layer normalization by\n",
    "computing the mean and variance used for normalization from all of the summed\n",
    "inputs to the neurons in a layer on a single training case. Like batch normalization,\n",
    "we also give each neuron its own adaptive bias and gain which are applied after\n",
    "the normalization but before the non-linearity. Unlike batch normalization, layer\n",
    "normalization performs exactly the same computation at training and test times.\n",
    "\n",
    "- It is also straightforward to apply to recurrent neural networks by computing the\n",
    "normalization statistics separately at each time step. \n",
    "\n",
    "- Layer normalization is very\n",
    "effective at stabilizing the hidden state dynamics in recurrent networks. \n",
    "\n",
    "- Layer normalization can substantially reduce the training time\n",
    "compared with previously published techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "# Activate module\n",
    "layer_norm(embedding)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout\n",
    "\n",
    "- Applied after any non-output layer\n",
    "- During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.\n",
    "- Not applied during eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Dropout(p=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings\n",
    "\n",
    "- An embedding is an efficient alternative to a single linear layer when one has a large number of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dddab157190c3c29c7bfa9724dd2612e80e7d4a281bb7f76e54f36d2e23abd8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
